

La reducción de dimensionalidad es una técnica utilizada en el análisis de datos y aprendizaje automático para reducir el número de variables o características en un conjunto de datos, manteniendo la mayor cantidad de información relevante posible. Esto es útil cuando se trabaja con datos de alta dimensionalidad, ya que puede ayudar a:

1. Reducir la complejidad del modelo
2. Disminuir el tiempo de cómputo
3. Evitar el sobreajuste
4. Facilitar la visualización de los datos

El PCA (Análisis de Componentes Principales, por sus siglas en inglés) es uno de los métodos más comunes para la reducción de dimensionalidad. El PCA funciona de la siguiente manera:

1. Identifica las direcciones (componentes principales) en las que los datos varían más.
2. Proyecta los datos sobre estas direcciones, creando nuevas variables que son combinaciones lineales de las originales.
3. Ordena estas nuevas variables (componentes principales) por la cantidad de varianza que explican.
4. Selecciona un subconjunto de los componentes principales más importantes, reduciendo así la dimensionalidad de los datos.

En Orange Data Mining, el widget PCA te permite aplicar este análisis a tus datos, visualizar los resultados y seleccionar cuántos componentes principales deseas conservar.

### PC1 y PC2

PC1 y PC2 son las dos primeras Componentes Principales obtenidas del Análisis de Componentes Principales (PCA). Vamos a entenderlas en más detalle:

1. PC1 (Primera Componente Principal):
   - Es la nueva variable que captura la mayor varianza en los datos.
   - Representa la dirección en el espacio de datos original donde los datos varían más.
   - Es una combinación lineal de las variables originales.

2. PC2 (Segunda Componente Principal):
   - Es la segunda nueva variable que captura la mayor varianza restante.
   - Es ortogonal (perpendicular) a PC1.
   - También es una combinación lineal de las variables originales, pero diferente a PC1.

Características importantes:

- PC1 y PC2 son nuevos "ejes" en el espacio de datos.
- Juntas, explican la mayor parte de la variabilidad en los datos.
- Son no correlacionadas entre sí.
- Cada una es una combinación ponderada de las variables originales.

En el contexto de los datos de Iris:
- PC1 podría representar una combinación de longitud y ancho de pétalo y sépalo que mejor distingue entre las especies de iris.
- PC2 podría capturar la segunda característica más distintiva, complementaria a PC1.

Estas nuevas variables te permiten visualizar y analizar los datos de Iris en un espacio reducido, potencialmente revelando patrones que no eran evidentes en las variables originales.

### ¿QUÉ ES LA DIMENSIONALIDAD?

En el contexto del análisis de datos y aprendizaje automático, la dimensionalidad se refiere al número de atributos o características (variables) que describen cada instancia de datos.

Para entenderlo mejor:

1. En un conjunto de datos:
   - Cada columna representa una dimensión o variable.
   - Cada fila representa una instancia o muestra.

2. Ejemplos:
   - En el conjunto de datos Iris, hay 4 dimensiones: longitud del sépalo, ancho del sépalo, longitud del pétalo y ancho del pétalo.
   - En un conjunto de datos de clientes, las dimensiones podrían ser edad, ingreso, historial de compras, etc.

3. Alta dimensionalidad:
   - Ocurre cuando hay muchas variables (por ejemplo, cientos o miles).
   - Puede llevar a problemas como la "maldición de la dimensionalidad", donde el espacio se vuelve muy disperso y el análisis más difícil.

4. Importancia:
   - Afecta la complejidad de los modelos de aprendizaje automático.
   - Influye en el tiempo de cómputo y la cantidad de datos necesarios.
   - Puede impactar en la interpretabilidad de los resultados.

5. En el contexto de PCA:
   - Buscamos reducir el número de dimensiones (variables) mientras mantenemos la mayor parte de la información.
   - Transformamos las dimensiones originales en un nuevo conjunto de dimensiones (componentes principales) que capturan la mayoría de la varianza en los datos.

Entender la dimensionalidad es crucial porque muchas técnicas de análisis de datos y aprendizaje automático se comportan de manera diferente en espacios de alta dimensionalidad versus baja dimensionalidad.
